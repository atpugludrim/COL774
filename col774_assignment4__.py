# -*- coding: utf-8 -*-
"""COL774 ASSIGNMENT4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K1bgZjdCGxlpJnWmS_HGkpAetOpWqqOy
"""

# !pip install gdown
# 
# !gdown https://drive.google.com/uc?id=1U4C_FYNNqERJ6UaJ86APOD02WV_6dKJw&export=download
# 
# !gdown https://drive.google.com/uc?id=1il_UCkzpMOfUFsTo6-IDU2SUbLi0SZ2N
# 
# !gdown https://drive.google.com/uc?id=1QXiu1hRzGYD7_KC_ke2aeqS2rezJPruz&export=download
# 
# !unzip train_data.zip
# !unzip test_data.zip
# 
# !du -h --max-depth 0 train_data
# 
# !pip install --upgrade imutils

# Commented out IPython magic to ensure Python compatibility.
#import pandas as pd
#import numpy as np
# import matplotlib.pyplot as plt
import cv2
import os
import math
import argparse
#from google.colab.patches import cv2_imshow
#from imutils.object_detection import non_max_suppression
# %matplotlib inline

parser = argparse.ArgumentParser()
parser.add_argument('--device',choices=['cpu','cuda'],required=True)
args = parser.parse_args()

#cv2_imshow(image)

"""Final architecture to implement**(?)**: **[Image text recognition using cnn-rnn](https://medium.com/analytics-vidhya/image-text-recognition-738a368368f5)**, another option is **[this](https://medium.com/capital-one-tech/learning-to-read-computer-vision-methods-for-extracting-text-from-images-2ffcdae11594)**

- Must process data compatible to this type

Refer these
- [Text bounding box detection](https://learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/)
> [Code](https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/) (github, also shows implementation for GPU)

- [EAST (Efficient and Accurate Scene Text detector) model](https://github.com/spmallick/learnopencv/blob/master/TextDetectionEAST/textDetection.py)
> [paper](https://arxiv.org/abs/1704.03155), in case we have to implement ourselves, or to add in reference

- [Show and tell code](https://github.com/karpathy/neuraltalk)
- [Papers with code - Scene text](https://paperswithcode.com/task/scene-text)

References from the assignment doc:
- [Show and tell](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf)
- [Pytorch Tutorials](https://pytorch.org/tutorials/)
- Refer to the starter code provided
- Use START and END tokens
- Use attention to look at parts of image
- Use Transfer Learning
- Use Beam Search
- [Image captioning tokenization ref](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)
"""

# !wget -O EAST.tar.gz https://www.dropbox.com/s/r2ingd0l3zt8hxs/frozen_east_text_detection.tar.gz?dl=1

# !tar xvzf EAST.tar.gz

def decode(scores,geometry,scoreThresh):
  detections = []
  confidences = []
  assert len(scores.shape) == 4, "Incorrect dimensions of scores"
  assert len(geometry.shape) == 4, "Incorrect dimensions of geometry"
  assert scores.shape[0] == 1, "Invalid dimensions of scores"
  assert geometry.shape[0] == 1, "Invalid dimensions of geometry"
  assert scores.shape[1] == 1, "Invalid dimensions of scores"
  assert geometry.shape[1] == 5, "Invalid dimensions of geometry"
  assert scores.shape[2] == geometry.shape[2], "Invalid dimensions of scores and geometry"
  assert scores.shape[3] == geometry.shape[3], "Invalid dimensions of scores and geometry"
  height = scores.shape[2]
  width = scores.shape[3]
  for y in range(0, height):
    scoresData = scores[0][0][y]
    x0_data = geometry[0][0][y]
    x1_data = geometry[0][1][y]
    x2_data = geometry[0][2][y]
    x3_data = geometry[0][3][y]
    anglesData = geometry[0][4][y]
    for x in range(0, width):
        score = scoresData[x]

        # If score is lower than threshold score, move to next x
        if(score < scoreThresh):
            continue

        # Calculate offset
        offsetX = x * 4.0
        offsetY = y * 4.0
        angle = anglesData[x]

        # Calculate cos and sin of angle
        cosA = math.cos(angle)
        sinA = math.sin(angle)
        h = x0_data[x] + x2_data[x]
        w = x1_data[x] + x3_data[x]

        # Calculate offset
        offset = ([offsetX + cosA * x1_data[x] + sinA * x2_data[x], offsetY - sinA * x1_data[x] + cosA * x2_data[x]])

        # Find points for rectangle
        p1 = (-sinA * h + offset[0], -cosA * h + offset[1])
        p3 = (-cosA * w + offset[0],  sinA * w + offset[1])
        center = (0.5*(p1[0]+p3[0]), 0.5*(p1[1]+p3[1]))
        detections.append((center, (w,h), -1*angle * 180.0 / math.pi))
        confidences.append(float(score))

# Return detections and confidences
  return [detections, confidences]

net = cv2.dnn.readNet('frozen_east_text_detection.pb')
if args.device == 'cpu':
    net.setPreferableBackend(cv2.dnn.DNN_TARGET_CPU)
    print("Using CPU")
elif args.device == 'cuda':
    net.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)
    net.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)
    print("Using CUDA")
# number = np.random.randint(1,50000)
# number=41784
max_ = len(os.listdir('test_data'))
for fnumber in range(1, max_+1):
    print("Loading test_data/test{}.jpg".format(fnumber))
    image = cv2.imread("test_data/test{}.jpg".format(fnumber),0)
    g2rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
    blob = cv2.dnn.blobFromImage(g2rgb,1.0,(320,320),(123.68,116.78,103.94),True,False)
    outputLayer = []
    outputLayer.append("feature_fusion/Conv_7/Sigmoid")
    outputLayer.append("feature_fusion/concat_3")
    net.setInput(blob)
    output = net.forward(outputLayer)
    scores=output[0]
    geometry=output[1]
    ct = 0.98
    nmst = 0.2
    [boxes, confidence] = decode(scores,geometry,ct)
    indices = cv2.dnn.NMSBoxesRotated(boxes, confidence, ct, nmst)

    try:
        print(indices)
        print(type(indices))
        print(type(boxes))
        print(len(boxes))
        print(type(boxes[0]))
        #print(len(boxes[0]))
        #print(type(boxes[0][0]))
        #print(len(boxes[0][0]))
        #print(boxes[0][0])
        #print(boxes[0][1])
        #print(boxes[1][0])
        #print(boxes[1][1])
    except:
        continue

    try:
        inpWidth, inpHeight = 320,320
        height_ = image.shape[0]
        width_ = image.shape[1]
        rW = width_ / float(inpWidth)
        rH = height_ / float(inpHeight)
        xboxmin = float('inf')
        xboxmax = -float('inf')
        yboxmin = float('inf')
        yboxmax = -float('inf')
        for i in indices:
            # get 4 corners of the rotated rect
            vertices = cv2.boxPoints(boxes[i])
            # scale the bounding box coordinates based on the respective ratios
            for j in range(4):
                vertices[j][0] *= rW
                vertices[j][1] *= rH
            for j in range(4):
                p1 = (vertices[j][0], vertices[j][1])
                if xboxmax < vertices[j][0]:
                    xboxmax = vertices[j][0]
                if xboxmin > vertices[j][0]:
                    xboxmin = vertices[j][0]
                if yboxmax < vertices[j][1]:
                    yboxmax = vertices[j][1]
                if yboxmin > vertices[j][1]:
                    yboxmin = vertices[j][1]
                p2 = (vertices[(j + 1) % 4][1], vertices[(j + 1) % 4][1])
                # cv2.line(image, p1, p2, (255, 255, 255), 2, cv2.LINE_AA)
                # cv.putText(frame, "{:.3f}".format(confidences[i[0]]), (vertices[0][0], vertices[0][1]), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv.LINE_AA)
    except:
        continue

    xboxmin = int(xboxmin)
    xboxmax = int(xboxmax)
    yboxmin = int(yboxmin)
    yboxmax = int(yboxmax)
    padding = 20
    xboxmin = max(0,xboxmin - padding)
    xboxmax = min(xboxmax + padding,image.shape[1])
    yboxmin = max(0,yboxmin - padding)
    yboxmax = min(yboxmax + padding,image.shape[0])
    cv2.imwrite('processed_test/test{}.jpg'.format(fnumber),image[yboxmin:yboxmax,xboxmin:xboxmax])
# Put efficiency information
#cv2.putText(image, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))

# Display the frame
# cv2_imshow(image)
