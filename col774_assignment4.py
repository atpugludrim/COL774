# -*- coding: utf-8 -*-
"""COL774 ASSIGNMENT4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K1bgZjdCGxlpJnWmS_HGkpAetOpWqqOy
"""

!pip install gdown

!gdown https://drive.google.com/uc?id=1U4C_FYNNqERJ6UaJ86APOD02WV_6dKJw&export=download

!gdown https://drive.google.com/uc?id=1il_UCkzpMOfUFsTo6-IDU2SUbLi0SZ2N

!gdown https://drive.google.com/uc?id=1QXiu1hRzGYD7_KC_ke2aeqS2rezJPruz&export=download

!unzip train_data.zip
!unzip test_data.zip

!du -h --max-depth 0 train_data

!pip install --upgrade imutils

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cv2
import math
from google.colab.patches import cv2_imshow
from imutils.object_detection import non_max_suppression
# %matplotlib inline

image = cv2.imread("train_data/res38531.jpg",0)
cv2_imshow(image)

"""Final architecture to implement**(?)**: **[Image text recognition using cnn-rnn](https://medium.com/analytics-vidhya/image-text-recognition-738a368368f5)**, another option is **[this](https://medium.com/capital-one-tech/learning-to-read-computer-vision-methods-for-extracting-text-from-images-2ffcdae11594)**

- Must process data compatible to this type

Refer these
- [Text bounding box detection](https://learnopencv.com/deep-learning-based-text-detection-using-opencv-c-python/)
> [Code](https://www.pyimagesearch.com/2018/08/20/opencv-text-detection-east-text-detector/) (github, also shows implementation for GPU)

- [EAST (Efficient and Accurate Scene Text detector) model](https://github.com/spmallick/learnopencv/blob/master/TextDetectionEAST/textDetection.py)
> [paper](https://arxiv.org/abs/1704.03155), in case we have to implement ourselves, or to add in reference

- [Show and tell code](https://github.com/karpathy/neuraltalk)
- [Papers with code - Scene text](https://paperswithcode.com/task/scene-text)

References from the assignment doc:
- [Show and tell](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf)
- [Pytorch Tutorials](https://pytorch.org/tutorials/)
- Refer to the starter code provided
- Use START and END tokens
- Use attention to look at parts of image
- Use Transfer Learning
- Use Beam Search
- [Image captioning tokenization ref](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)
"""

!wget -O EAST.tar.gz https://www.dropbox.com/s/r2ingd0l3zt8hxs/frozen_east_text_detection.tar.gz?dl=1

!tar xvzf EAST.tar.gz

def decode(scores,geometry,scoreThresh):
  detections = []
  confidences = []
  assert len(scores.shape) == 4, "Incorrect dimensions of scores"
  assert len(geometry.shape) == 4, "Incorrect dimensions of geometry"
  assert scores.shape[0] == 1, "Invalid dimensions of scores"
  assert geometry.shape[0] == 1, "Invalid dimensions of geometry"
  assert scores.shape[1] == 1, "Invalid dimensions of scores"
  assert geometry.shape[1] == 5, "Invalid dimensions of geometry"
  assert scores.shape[2] == geometry.shape[2], "Invalid dimensions of scores and geometry"
  assert scores.shape[3] == geometry.shape[3], "Invalid dimensions of scores and geometry"
  height = scores.shape[2]
  width = scores.shape[3]
  for y in range(0, height):
    scoresData = scores[0][0][y]
    x0_data = geometry[0][0][y]
    x1_data = geometry[0][1][y]
    x2_data = geometry[0][2][y]
    x3_data = geometry[0][3][y]
    anglesData = geometry[0][4][y]
    for x in range(0, width):
        score = scoresData[x]

        # If score is lower than threshold score, move to next x
        if(score < scoreThresh):
            continue

        # Calculate offset
        offsetX = x * 4.0
        offsetY = y * 4.0
        angle = anglesData[x]

        # Calculate cos and sin of angle
        cosA = math.cos(angle)
        sinA = math.sin(angle)
        h = x0_data[x] + x2_data[x]
        w = x1_data[x] + x3_data[x]

        # Calculate offset
        offset = ([offsetX + cosA * x1_data[x] + sinA * x2_data[x], offsetY - sinA * x1_data[x] + cosA * x2_data[x]])

        # Find points for rectangle
        p1 = (-sinA * h + offset[0], -cosA * h + offset[1])
        p3 = (-cosA * w + offset[0],  sinA * w + offset[1])
        center = (0.5*(p1[0]+p3[0]), 0.5*(p1[1]+p3[1]))
        detections.append((center, (w,h), -1*angle * 180.0 / math.pi))
        confidences.append(float(score))

# Return detections and confidences
  return [detections, confidences]

net = cv2.dnn.readNet('frozen_east_text_detection.pb')
g2rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
blob = cv2.dnn.blobFromImage(g2rgb,1.0,(320,320),(123.68,116.78,103.94),True,False)
outputLayer = []
outputLayer.append("feature_fusion/Conv_7/Sigmoid")
outputLayer.append("feature_fusion/concat_3")
net.setInput(blob)
output = net.forward(outputLayer)
scores=output[0]
geometry=output[1]
ct = 0.98
nmst = 0.2
[boxes, confidence] = decode(scores,geometry,ct)
indices = cv2.dnn.NMSBoxesRotated(boxes, confidence, ct, nmst)

print(indices)
print(type(indices))
print(type(boxes))
print(len(boxes))
print(type(boxes[0]))
print(len(boxes[0]))
print(type(boxes[0][0]))
print(len(boxes[0][0]))
print(boxes[0][0])
print(boxes[0][1])
print(boxes[1][0])
print(boxes[1][1])

inpWidth, inpHeight = 320,320
height_ = image.shape[0]
width_ = image.shape[1]
rW = width_ / float(inpWidth)
rH = height_ / float(inpHeight)
for i in indices:
    # get 4 corners of the rotated rect
    vertices = cv2.boxPoints(boxes[i[0]])
    # scale the bounding box coordinates based on the respective ratios
    for j in range(4):
        vertices[j][0] *= rW
        vertices[j][1] *= rH
    for j in range(4):
        p1 = (vertices[j][0], vertices[j][1])
        p2 = (vertices[(j + 1) % 4][0], vertices[(j + 1) % 4][1])
        cv2.line(image, p1, p2, (255, 255, 255), 2, cv2.LINE_AA)
        # cv.putText(frame, "{:.3f}".format(confidences[i[0]]), (vertices[0][0], vertices[0][1]), cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv.LINE_AA)

# Put efficiency information
#cv2.putText(image, label, (0, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255))

# Display the frame
cv2_imshow(image)

!head Train_text.tsv

"""# BASIC CNN MODEL"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, utils

from skimage import io, transform

import matplotlib.pyplot as plt
import numpy as np

class Rescale:
  def __init__(this, output_size):
    assert isinstance(output_size, (int, tuple))
    this.output_size = output_size
  def __call__(this, image):
    h, w = image.shape[:2]
    if isinstance(this.output_size, int):
      if h > w:
        new_h, new_w = this.output_size * h / w, this.output_size
      else:
        new_h, new_w = this.output_size, this.output_size * w / h
    else:
      new_h, new_w = this.output_size
    
    new_h, new_w = int(new_h), int(new_w)
    img = transform.resize(image, (new_h, new_w))
    return img

class ToTensor:
  def __call__(this, image):
    # numpy_image: H x W x C (C is color)
    # torch imagae: C x H x W
    image = image.transpose((2,0,1))
    return image

IMAGE_RESIZE = (256,256)
img_transform = transforms.Compose([Rescale(IMAGE_RESIZE), ToTensor()])

class CaptionsPreprocessing:
  """Preprocess the captions, generate vocab and convert words to tensor tokens
  """
  def __init__(this, captions_file_path):
    this.captions_file_path = captions_file_path
    this.raw_captions_dict = this.read_raw_captions()
    this.captions_dict = this.process_captions()
    this.vocab = this.generate_vocabulary()
  def read_raw_captions(this):
    """Returns dictionary with raw captions list keyed by image ids (integers)"""
    captions_dict = {}
    with open(this.captions_file_path,"r",encoding='utf-8') as f:
      for img_caption_line in f:
        img_captions = img_caption_line.strip().split('\t')
        captions_dict[img_captions[0]] = img_captions[1]
    return captions_dict
  
  def process_captions(this):
    """ Generate dictionary and other preprocessing"""
    raw_captions_dict = this.raw_captions_dict
    
    # DO THE PROCESSING HERE
    captions_dict = raw_captions_dict
    return captions_dict
  
  def generate_vocabulary(this):
    """ Generate dictionary and other preprocessing"""
    captions_dict = this.captions_dict
    # Generate vocabulary
    return None
  
  def captions_transform(this, img_captions_list):
    """ Generate tensor tokens for the text captions"""
    vocab = this.vocab
    
    # Gnerate tensors

    return torch.zeros(len(img_captions_list),10)

def proc_capt_prox(this):
  raw_captions_dict = this.raw_captions_dict
  captions_dict = raw_captions_dict
  for k in captions_dict:
    captions_dict[k] = 'xxstart '+captions_dict[k]+' xxend'
  return captions_dict

def gen_vocab_proxy(this):
  captions_dict = this.captions_dict
  words = dict()
  for k in captions_dict:
    for token in captions_dict[k].strip().split():
      words[token] = 1
  vocab = {k: v+2 for v, k in enumerate(words)}
  vocab['xxstart'] = 0
  vocab['xxend'] = 1
  return vocab
  
CaptionsPreprocessing.process_captions = proc_capt_prox
CaptionsPreprocessing.generate_vocabulary = gen_vocab_proxy

CAPTIONS_FILE_PATH = 'Train_text.tsv'
captions_preprocessing_obj = CaptionsPreprocessing(CAPTIONS_FILE_PATH)

class ImageCaptionsDataset(Dataset):
  def __init__(this, img_dir, captions_dict, img_transform=None, captions_transform=None):
    this.img_dir = img_dir
    this.captions_dict = captions_dict
    this.img_transform = img_transform
    this.captions_transform = captions_transform

    this.image_ids = list(captions_dict.keys())
  def __len__(this):
    return len(this.image_ids)
  
  def __getitem__(this, idx):
    img_name = this.image_ids[idx]
    image = io.imread(img_name)
    captions = this.captions_dict[img_name]
    if this.img_transform:
      image = this.img_transform(image)
    if this.captios_transform:
      captions = this.captions_tranform(captions)
    
    sample = {'image':image,'captions':captions}

    return sample

class ImageCaptionsNet(nn.Module):
  def __init__(this):
    super(ImageCaptionsNet, this).__init__()
    # Define architecture here
  def forward(this, x):
    x = image_batch, captions_batch # ??
    # Forward propagations
    return captions_batch

if torch.cuda.is_available():
  device = torch.device('cuda')
else:
  device = torch.device('cpu')
net = ImageCaptionsNet()
net = net.to(device)

IMAGE_DIR = ''

train_dataset = ImageCaptionsDataset(IMAGE_DIR, captions_preprocessing_obj.captions_dict, img_transform = img_transform, captions_tranform=captions_processing_obj.captions_transform)
NUMBER_OF_EPOCHS = 3
LEARNING_RATE = 1e-1
BATCH_SIZE = 32
NUM_WORKERS = 0 # Parallel threads for dataloading
loss_function = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)
train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)

import os

for epoch in range(NUMBER_OF_EPOCHS):
  for batch_idx, sample in enumerate(train_loader):
    optimizer.zero_grad()
    image_batch, captions_batch = sample['image'], sample['captions']
    image_batch, captions_batch = image_batch.to(device), captions_batch.to(device)
    output_captions = net((image_batch, captions_batch))
    loss = loss_function(output_captions, captions_batch)
    loss.backward()
    optimizer.step()
  print("Iteration: ",epoch+1)