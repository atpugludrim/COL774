% Created 2021-11-25 Thu 12:30
\documentclass[11pt]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
%\usepackage{wrapfig}
\usepackage{rotating}
%\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
%\usepackage{marvosym}
%\usepackage{wasysym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\author{Suchith (2021AIZ8323) and Mridul (2021AIZ8322)}
\date{}
\title{COL774 Assignment 4}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs 25.2.2 (Org mode 8.2.10)}}
\begin{document}

\maketitle
\setcounter{section}{1}
\section{Non-competitive part}
\label{sec-1}
\subsection{Pre-processing}
\label{sec-1-1}
\begin{itemize}
\item Building on the \texttt{starter\_code.py}, the captions were first
enclosed within START and END tokens, and then padded with PAD
tokens to make them all fixed length. Let's call this fixed length \(\ell\label{ell}\).
\item The captions were tokenized at word-level. Character-level
tokenization with OCR task was considered, but we didn't have the
right dataset to train an OCR like model.
\item After tokenization, they were converted into one-hot vectors. There
were 7739 tokens.
\item Images were resized to fixed size. Thresholding the images to remove
the background details and converting to grayscale was tried, it
didn't help.
\item In images, we used the EAST text detector in opencv to crop tightly
to the text. The results were poor.
\item Som in conclusion, the captions were tokenized, enclosed in start
and end markers and padded to fixed length \(\ell\) after which a vocabulary
mapped it to integers which was then mapped to one-hot
encodings. Images were resized to fixed width-height.
\end{itemize}
\subsection{CNN Encoder}
\label{sec-1-2}
We made a simple CNN block to extract features from images
comprising of convolutional layers and pooling layers. Default kernel
size and stride size were used.
\subsection{RNN Decoder}
\label{sec-1-3}
\begin{itemize}
\item The output of the CNN decoder was first fed into a linear layer to
create attention weights. It was then multiplied with features
output by encoder.
\item This was then passed into two other linear layers which would output
initial hidden state and cell state to be used by RNN decoder.
\item We used \texttt{torch.nn.LSTM} in biderectional mode first, 2 layers
deep. We soon realized a problem here: this only gave the final
output. So, we switched to \texttt{torch.nn.LSTMCell} that would
give out the hidden and cell states at each time step and it can be
used to run it for as many time steps as needed.
\item So, in the \texttt{torch.nn.Module.forward} method (inherited by out
model) we had a \texttt{for} loop running for the length of the
sequence which was the same as \(\ell\) (see page \pageref{ell}).
\item The hidden state at each step is seen as the unnormalized log
probabilities over all possible words.
\item We did teacher-forcing, just as asked for.
\end{itemize}
\subsection{Loss}
\label{sec-1-4}
We used negative log likelihood loss to maximize the likelihood
of each word at each time step for each example. So,
\begin{align*}
P_v(x^{(i)})&=[\operatorname{softmax}(h_\theta(x^{(i)}))]_v, \; v\in V\\
LL(\theta;x,y)&=\sum_{i\in M}\sum_{t=1}^T\sum_{v\in V} 1\{y^{(i)}_t=v\}\log{P_v(x^{(i)})}
\end{align*}
where with abuse of notation, \(h_\theta(x^{(i)})\) is used to mean
the hidden state output by the rnn and time \(t\) by the RNN
decoder. \(V\) is the set of all words, the vocabulary.\par
The parameters of the model, both encoder and decoder are then trained
in an end-to-end fashion using SGD optimizer.
% Emacs 25.2.2 (Org mode 8.2.10)
\end{document}
