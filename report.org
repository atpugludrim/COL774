* Q1

** Q1(a)
- Punctutations were removed from the input data, and words containing
  numbers like l77t code were also removed.
- The bag of words model was implemented.
- Optimal parameters are stored in a pickle file.
- Train accuracy is \(71.25\%\) while test accuracy is \(66.54\%\).

** Q1(b)
- Expected random prediction accuracy is \(20\%\). Predicting the
  maximum class, the accuracy will be \(66.086\%\).
- As compared to random prediction, the accuracy gained is \(36.08\%\),
  but there is no significant gain compared to maximum prediction.

** Q1(c)
#+CAPTION: Confusion matrix for Q1(a)
[[/home/mridul/scai/ml/hw2/src/q1/Confusion_Matrix_1a.png]]
- Class "5" has the highest value of the diagonal entry. This is also
  the class with highest number of samples in the training set.
- It is also seen that classes that have a high prior, pull the
  decision towards themselves: most of the false predictions belong to
  class "5", then to class "4" and so on. The number of training
  samples from each class is listed below.
- Summing up the column for class "5", we can see 11984 = 85.6\% of
  samples are predicted as belonging to class "5". 13.44\% to class
  "4", 0.8\% to class "3", 0\% to class "2" and 0.157 to class "1".
\begin{align*}
&\text{Category 1:} 2529 =5.05\%\\
&\text{Category 2:} 2638 =5.28\%\\
&\text{Category 3:} 5634 =11.27\%\\
&\text{Category 4:} 13267 =26.53\%\\
&\text{Catgeory 5:} 25932 =51.86\%
\end{align*}

** Q1(d)
#+CAPTION: Confusion matrix for Q1(d)
[[/home/mridul/scai/ml/hw2/src/q1/Confusion_Matrix_1d.png]]
- Accuracy on test set is \(64.77\%\).
- One would expect the accuracy to improve, but it actually goes down
  by 1.77\%.

* Q2
 
** Q2(a)
*** i)
Accuracy on training data is \(99.28\%\). Accuracy on test data is
\(97.26\%\). There are 309 support vectors in this case.

*** ii)
Accuracy on training data is \(100\%\), while on test data is
\(99.02\%\). There are 1856 support vectors in this case. The accuracy
improves as compared to when using linear kernel, but so does the
computation time. And in this particular case, the gain in accuracy is
not enough to justify the loss in computation time.

*** iii)
- linear: nSV = 297. Test set accuracy is \(97.01\%\). Training and
  testing times are 10\(\times\) faster than my cvxopt implementation.
- gaussian: nSV = 1823. Test set accuracy is \(99.26\%\). Training and
  testing times are 20\(\times\) faster than my cvxopt implmentation.
- As noted [[https://stackoverflow.com/a/5333279][here]], it is not possible to extract weights and bias from
  the svm model created by libsvm in python. I did not compare them.

** Q2(b)
*** i)
Test set accuracy is \(96.84\%\). The testing process is very slow
though, takes around 15 minutes on the test set of \(10^4\)
samples. Training time is around 70 minutes.

*** ii)
Test set accuracy is \(97.23\%\). The accuracy is not significantly
different from the cvxopt implementation. Without any information
about SVM performance on the given dataset other than the cvxopt
implementation, the expected accuracy is \(96.84\%\). According to
Markov inequality then, \(\displaystyle P[\text{acc}\ge 97.23]\le
\frac{96.84}{97.23}=0.9959\). That is to sat, this is not such a rare event.

\par The testing time is 1.5 minutes. Training time is around 3
minutes. Around 25 times faster.

*** iii)
#+CAPTION: Confusion matrix for o-v-o SVM using CVXOPT
[[/home/mridul/scai/ml/hw2/src/q2/cvxopt_multi.png]]
#+CAPTION: Confusion matrix for o-v-o SVM using LIBSVM
[[/home/mridul/scai/ml/hw2/src/q2/libsvm_multi.png]]
#+CAPTION: Misclassified images by SVM using CVXOPT
[[/home/mridul/scai/ml/hw2/src/q2/cvxopt.png]]
#+CAPTION: Misclassified imaged by SVM using LIBSVM
[[/home/mridul/scai/ml/hw2/src/q2/libsvm.png]]
The images misclassified by the model are confusing to my eyes as
well. Some of them are written in poor handwriting or incomplete, and
trying to complete them one can go two ways, the predicted and the
true label.
\par
For cvxopt\\
0 is usually confused as 6
1 is confused as 2, 3, 4, 6, 3 and 9\\
2 mostly with 8, 7 and 3.\\
3 as 2, 7 and 8.\\
4 as 9.\\
5 as 6, 3 and 8.\\
6 as 0.\\
7 as 2, 1 and 9.\\
8 as 3\\
9 as 4 and 8.\\
Similar trends hold for libsvm implementation as well.

*** iv)
