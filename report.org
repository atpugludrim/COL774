* Q1

** Q1(a)
- Punctutations were removed from the input data, and words containing
  numbers like l77t code were also removed.
- The bag of words model was implemented.
- Optimal parameters are stored in a pickle file.
- Train accuracy is \(71.25\%\) while test accuracy is \(66.54\%\).

** Q1(b)
- Expected random prediction accuracy is \(20\%\). Predicting the
  maximum class, the accuracy will be \(66.086\%\).
- As compared to random prediction, the accuracy gained is \(36.08\%\),
  but there is no significant gain compared to maximum prediction.

** Q1(c)
#+CAPTION: Confusion matrix for Q1(a)
[[/home/mridul/scai/ml/hw2/src/q1/Confusion_Matrix_1a.png]]
- Class "5" has the highest value of the diagonal entry. This is also
  the class with highest number of samples in the training set.
- It is also seen that classes that have a high prior, pull the
  decision towards themselves: most of the false predictions belong to
  class "5", then to class "4" and so on. The number of training
  samples from each class is listed below.
- Summing up the column for class "5", we can see 11984 = 85.6\% of
  samples are predicted as belonging to class "5". 13.44\% to class
  "4", 0.8\% to class "3", 0\% to class "2" and 0.157 to class "1".
\begin{align*}
&\text{Category 1:} 2529 =5.05\%\\
&\text{Category 2:} 2638 =5.28\%\\
&\text{Category 3:} 5634 =11.27\%\\
&\text{Category 4:} 13267 =26.53\%\\
&\text{Catgeory 5:} 25932 =51.86\%
\end{align*}

** Q1(d)
#+CAPTION: Confusion matrix for Q1(d)
[[/home/mridul/scai/ml/hw2/src/q1/Confusion_Matrix_1d.png]]
- Accuracy on test set is \(64.77\%\).
- One would expect the accuracy to improve, but it actually goes down
  by 1.77\%.

* Q2
 
** Q2(a)
*** i)
Accuracy on training data is \(99.28\%\). Accuracy on test data is
\(97.26\%\). There are 309 support vectors in this case.

*** ii)
Accuracy on training data is \(100\%\), while on test data is
\(99.02\%\). There are 1856 support vectors in this case. The accuracy
improves as compared to when using linear kernel, but so does the
computation time. And in this particular case, the gain in accuracy is
not enough to justify the loss in computation time.

*** iii)
- linear: nSV = 297. Test set accuracy is \(97.01\%\). Training and
  testing times are 10\(\times\) faster than my cvxopt implementation.
- gaussian: nSV = 1823. Test set accuracy is \(99.26\%\). Training and
  testing times are 20\(\times\) faster than my cvxopt implmentation.
- As noted [[https://stackoverflow.com/a/5333279][here]], it is not possible to extract weights and bias from
  the svm model created by libsvm in python. I did not compare them.

** Q2(b)
*** i)
Test set accuracy is \(96.84\%\). The testing process is very slow
though, takes around 15 minutes on the test set of \(10^4\),000 samples.

*** ii)
*** iii)
*** iv)
