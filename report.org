#+AUTHOR: Mridul Gupta \\ 2021AIZ8322
#+OPTION: toc:nil
* Q1

** Q1(a)
- Punctutations were removed from the input data, and words containing
  numbers like l77t code were also removed.
- The bag of words model was implemented.
- Optimal parameters are stored in a pickle file.
- Train accuracy is \(71.25\%\) while test accuracy is \(66.54\%\).
- Also, note that I tried to improve the running time to the point I
  wrote several different versions of the code, but it just doesn't
  get better than 30-40 mins. I hope this counts as \lq\lq
  reasonable\rq\rq\ time.

** Q1(b)
- Expected random prediction accuracy is \(20\%\). Predicting the
  maximum class, the accuracy will be \(66.086\%\).
- As compared to random prediction, the accuracy gained is \(36.08\%\),
  but there is no significant gain compared to maximum prediction.

** Q1(c)
#+CAPTION: Confusion matrix for Q1(a)
[[/home/mridul/scai/ml/hw2/src/q1/Confusion_Matrix_1a.png]]
- Class "5" has the highest value of the diagonal entry. This is also
  the class with highest number of samples in the training set.
- It is also seen that classes that have a high prior, pull the
  decision towards themselves: most of the false predictions belong to
  class "5", then to class "4" and so on. The number of training
  samples from each class is listed below.
- Summing up the column for class "5", we can see 11984 = 85.6\% of
  samples are predicted as belonging to class "5". 13.44\% to class
  "4", 0.8\% to class "3", 0\% to class "2" and 0.157 to class "1".
\begin{align*}
&\text{Category 1:} 2529 =5.05\%\\
&\text{Category 2:} 2638 =5.28\%\\
&\text{Category 3:} 5634 =11.27\%\\
&\text{Category 4:} 13267 =26.53\%\\
&\text{Catgeory 5:} 25932 =51.86\%
\end{align*}

** Q1(d)
#+CAPTION: Confusion matrix for Q1(d)
[[/home/mridul/scai/ml/hw2/src/q1/Confusion_Matrix_1d.png]]
- Accuracy on test set is \(64.77\%\).
- One would expect the accuracy to improve, but it actually goes down
  by 1.77\%.

** Q1(e)
#+CAPTION: Frequencies of words
[[/home/mridul/scai/ml/hw2/src/q1/freq_boxplot.png]]
Features used was review lengths discretized to four buckets based on
boxplot of classwise lengths plotted. Also, since most of the reviews
have \(<400\) words after removal of stop words but some reviews go on
to contain around 2500 words. We have a lot of data, so in this step I
do not process examples that contain more than 400 words. This can be
seen as a meta feature, to avoid overfitting. I tested with n-gram
extractions, but it was taking a lot of time to build the vocabulary,
so I had to terminate the code. The accuracy is no still around
\(65\%\) but there is some speed gain as we are not processing long
reviews.

** Q1(f)
The class-wise F1-scores and macro F1-score are:
\noindent\begin{tabular}{cc}
Class 1 & 26.95\%\\
Class 2 & 4.70\%\\
Class 3 & 18.77\%\\
Class 4 & 34.54\%\\
Class 5 & 80.36\%\\
Macro F1 & 33.07\%
\end{tabular}
The macro F1-score and the per class F1-scores is more suited in this
kind of dataset as the dataset is imabalanced, and this metric gives
insights into the performance per class. An extreme case might be of
cancer cell detection from images, where we might have very few
examples of the positive class. But it will be crucial to classify
them correctly so that a patient gets correct treatment on time and a
healthy person doesn't has to go through the treatment.


** Q1(g)
* Q2
 
** Q2(a)
*** i)
Accuracy on training data is \(99.28\%\). Accuracy on test data is
\(97.26\%\). There are 309 support vectors in this case.

*** ii)
Accuracy on training data is \(100\%\), while on test data is
\(99.02\%\). There are 1856 support vectors in this case. The accuracy
improves as compared to when using linear kernel, but so does the
computation time. And in this particular case, the gain in accuracy is
not enough to justify the loss in computation time.

*** iii)
- linear: nSV = 297. Test set accuracy is \(97.01\%\). Training and
  testing times are 10\(\times\) faster than my cvxopt implementation.
- gaussian: nSV = 1823. Test set accuracy is \(99.26\%\). Training and
  testing times are 20\(\times\) faster than my cvxopt implmentation.
- As noted [[https://stackoverflow.com/a/5333279][here]], it is not possible to extract weights and bias from
  the svm model created by libsvm in python. I did not compare them.

** Q2(b)
*** i)
Test set accuracy is \(96.84\%\). The testing process is very slow
though, takes around 15 minutes on the test set of \(10^4\)
samples. Training time is around 70 minutes.

*** ii)
Test set accuracy is \(97.23\%\). The accuracy is not significantly
different from the cvxopt implementation. Without any information
about SVM performance on the given dataset other than the cvxopt
implementation, the expected accuracy is \(96.84\%\). According to
Markov inequality then, \(\displaystyle P[\text{acc}\ge 97.23]\le
\frac{96.84}{97.23}=0.9959\). That is to sat, this is not such a rare event.

\par The testing time is 1.5 minutes. Training time is around 3
minutes. Around 25 times faster.

*** iii)
#+CAPTION: Confusion matrix for o-v-o SVM using CVXOPT
[[/home/mridul/scai/ml/hw2/src/q2/cvxopt_multi.png]]
#+CAPTION: Confusion matrix for o-v-o SVM using LIBSVM
[[/home/mridul/scai/ml/hw2/src/q2/libsvm_multi.png]]
#+CAPTION: Misclassified images by SVM using CVXOPT
[[/home/mridul/scai/ml/hw2/src/q2/cvxopt.png]]
#+CAPTION: Misclassified imaged by SVM using LIBSVM
[[/home/mridul/scai/ml/hw2/src/q2/libsvm.png]]
The images misclassified by the model are confusing to my eyes as
well. Some of them are written in poor handwriting or incomplete, and
trying to complete them one can go two ways, the predicted and the
true label.
\par
For cvxopt\\
0 is usually confused as 6
1 is confused as 2, 3, 4, 6, 3 and 9\\
2 mostly with 8, 7 and 3.\\
3 as 2, 7 and 8.\\
4 as 9.\\
5 as 6, 3 and 8.\\
6 as 0.\\
7 as 2, 1 and 9.\\
8 as 3\\
9 as 4 and 8.\\
Similar trends hold for libsvm implementation as well.

*** iv)
Value of C that gives best accuracy for 5-fold cross-validation
is 5. This is also the value of C that gives the best accuracy for
test set.\par
The graph shows that even though there is some variation, the 5-fold
cross-validation accuracies and the test set follow each other closely.
#+CAPTION: Cross-validation versus test set accuracies
[[/home/mridul/scai/ml/hw2/src/q2/kfold_cross_validation.png]]

